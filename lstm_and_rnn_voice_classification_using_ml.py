# -*- coding: utf-8 -*-
"""LSTM and RNN Voice Classification using ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11CyMTbCeHmymVbUeCN-ld6GXub1AL-jU
"""

import numpy as np
import pandas as pd
import os
import librosa
import wave
import matplotlib.pyplot

#MLP Classifier

from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

#LSTM Classifier
import keras
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import *
from keras.optimizer_v1 import rmsprop

from google.colab import drive
drive.mount('/content/drive')

def extract_mfcc(wav_file_name):
  y,sr=librosa.load(wav_file_name)
  mfcc=np.mean(librosa.feature.mfcc(y=y,sr=sr,n_mfcc=40).T,axis=0)
  return mfcc

#Load speech Dataset
radvess_speech_labels=[]
ravdess_speech_data=[]
for dirname, _ ,filenames in os.walk('/content/drive/MyDrive/Colab Notebooks/archive (1)'):
   for filename in filenames:
     #print(os.path.join(dirname,filename))
     radvess_speech_labels.append(int(filename[7:8]) - 1) # The indix 7 and 8 of the filename represents the emotion label
     wav_file_name=os.path.join(dirname,filename)
     ravdess_speech_data.append(extract_mfcc(wav_file_name))
print("Finish loading the dataset")

ravdess_speech_data

#Convert data  and label to array 
ravdess_speech_data_array=np.asarray(ravdess_speech_data) #Conver the input to the array
ravdess_speech_label_array=np.array(radvess_speech_labels)
ravdess_speech_label_array.shape #get tuple of array dimensions

#make categorical labels 
labels_categorical=to_categorical(ravdess_speech_label_array) #converts a class vector (integers) to binary class matrix
labels_categorical.shape

ravdess_speech_data_array.shape

x_train,x_test,y_train,y_test=train_test_split(np.array(ravdess_speech_data_array), labels_categorical, test_size=0.20,random_state=9)

from sklearn.utils import validation
#Split the training,validating and testing sets
number_of_samples=ravdess_speech_data_array.shape[0]
training_samples=int(number_of_samples*0.8)
validation_samples=int(number_of_samples*0.1)
test_samples=int(number_of_samples*0.1)

#define the LSTM model 
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import LSTM
def create_model_LSTM():
  model = Sequential()
  model.add(LSTM(128,return_sequences=False,input_shape=(40,1)))
  model.add(Dense(64))
  model.add(Dropout(0.4))
  model.add(Activation('relu'))
  model.add(Dense(32))
  model.add(Dropout(0.4))
  model.add(Activation('relu'))
  model.add(Dense(8))
  model.add(Activation('softmax'))

  #configure the model for taining
  model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])
  return model

w= np.expand_dims(ravdess_speech_data_array[:training_samples], -1)

w.shape

#train using LSTM Model
model_A = create_model_LSTM()
history=model_A.fit(np.expand_dims(ravdess_speech_data_array[:training_samples], -1), labels_categorical[:training_samples], validation_data=(np.expand_dims(ravdess_speech_data_array[training_samples:training_samples+validation_samples], -1),labels_categorical[training_samples:training_samples+validation_samples]), epochs=130,shuffle=True)

#Loss plots using LSTM model
import matplotlib.pyplot as plt

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs=range(1,len(loss)+1)

plt.plot(epochs,loss,'ro',label='Taining loss')
plt.plot(epochs,val_loss,'b',label='Validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#Accuracy plots using LSTM Model
acc=history.history['accuracy']
val_acc=history.history['val_accuracy']

plt.plot(epochs,acc,'ro',label='Training acc')
plt.plot(epochs,val_acc,'b',label='Validation acc')
plt.title('Taining and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#Evaluate using Model A
model_A.evaluate(np.expand_dims(ravdess_speech_data_array[training_samples+validation_samples:], -1),labels_categorical[training_samples + validation_samples:])

emotions = {1:'neutral',2:'calm',3:'happy',4:'sad',5:'angry',6:'fearful',7:'disgust',8:'suprised'}
def predict(wav_filepath):
  test_point=extract_mfcc(wav_filepath)
  test_point=np.reshape(test_point,newshape=(1,40,1))
  predictions=model_A.predict(test_point)
  print(emotions[np.argmax(predictions[0])+1])

predict('/content/drive/MyDrive/Colab Notebooks/archive (1)/Actor_03/03-01-01-01-01-01-03.wav')

predict('/content/drive/MyDrive/Colab Notebooks/archive (1)/Actor_16/03-01-03-01-01-02-16.wav')

predict('/content/drive/MyDrive/Colab Notebooks/archive (1)/Actor_21/03-01-02-01-02-02-21.wav')

model_A.save('mymodel.h5')

modelc=tf.keras.models.load_model('mymodel.h5')